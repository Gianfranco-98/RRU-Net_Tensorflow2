{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RRU_Net.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSGe0MxEMypA"
      },
      "source": [
        "## VP project - Gianfranco Di Marco - 1962292\n",
        "\\\n",
        "**RRU-Net: The Ringed Residual U-Net for Image Splicing Forgery Detection**\n",
        "*   X. Bi, Y. Wei, B. Xiao and W. Li, \"RRU-Net: *The Ringed Residual U-Net for Image Splicing Forgery Detection*\" 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019, pp. 30-39, doi: 10.1109/CVPRW.2019.00010.\n",
        "\n",
        "\n",
        "\\\n",
        "This project is an implementation in Tensorflow 2.0 of the above paper with a new Dataset: **Splicing COCO Dataset** (named by the author \"Spliced Nist\"); this can be found at the following link:\n",
        "\n",
        "> https://github.com/jawadbappy/forgery_localization_HLED/tree/master/synthetic_data\n",
        "\n",
        "\\\n",
        "The images of this dataset have been obtained from the pristine images of the MFC18 challenge, spliced with an object from the COCO Dataset. In particular, for this project, I used 180 images in total: 125 for the training set, 10 for the validation set and 45 for the test set. Here are the main citations for this data:\n",
        "\n",
        "*   Bappy, Jawadul H and Simons, Cody and Nataraj, Lakshmanan and Manjunath, BS and Roy-Chowdhury, Amit K, \"*Hybrid LSTM and Encoder-Decoder Architecture for Detection of Image Forgeries*\", IEEE Transactions on Image Processing, 2019\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbqL9oSVPe_F"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ReP2dVRDHaO"
      },
      "source": [
        "**Repository**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m14zZdBsDGp6"
      },
      "source": [
        "!git clone https://github.com/Gianfranco-98/RRU-Net_Tensorflow2.git\n",
        "!mv RRU-Net_Tensorflow2/* /content\n",
        "!rm LICENSE\n",
        "!rm README.md   \n",
        "!rm -r RRU-Net_Tensorflow2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mFM8zmVQ94A"
      },
      "source": [
        "**Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdhU8fGyAvWa"
      },
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade albumentations\n",
        "!apt-get update\n",
        "!apt install imagemagick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uobBYcqXQ_Wy"
      },
      "source": [
        "# Parameters\n",
        "from configuration import *\n",
        "\n",
        "# Dataset\n",
        "from dataset import *\n",
        "\n",
        "# Math\n",
        "import numpy as np\n",
        "from random import choice\n",
        "\n",
        "# Images\n",
        "import cv2\n",
        "import skimage\n",
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Learning\n",
        "from net import *\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# File management\n",
        "import os\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "\n",
        "# Time management\n",
        "import time\n",
        "\n",
        "# Other tools\n",
        "from functools import partial\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjsTdIzaDrOq"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTqI1vfs6lch"
      },
      "source": [
        "**Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUhDD7ZPDq_v"
      },
      "source": [
        "if DATASET_NAME == \"Realistic_Tampering_Dataset\":\n",
        "\n",
        "    # 0. Initialization\n",
        "    DATA_DIR = \"./data\"\n",
        "    PRISTINE_FOLDER = \"Pristine\"\n",
        "    FORGERED_FOLDER = \"Forgered\"\n",
        "    GROUND_TRUTH_FOLDER = \"Ground_Truth\"\n",
        "    DATASET_DIR = '/content/drive/MyDrive/VP_Project/data/' + DATASET_NAME\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 1. Directory creation\n",
        "    %cd /content\n",
        "    !mkdir data\n",
        "\n",
        "    # 2. Extraction\n",
        "    %cd $DATASET_DIR\n",
        "    print(\"Dataset extraction...\")\n",
        "    ZIPFILE = DATASET_NAME + '.zip'\n",
        "    !unzip $ZIPFILE -d /content/data\n",
        "\n",
        "    # 3. Conversion TIFF (PNG) -> JPEG with 100% quality\n",
        "    %cd /content/data\n",
        "    print(\"Conversion to JPEG with 100% quality\")\n",
        "    %cd $DATASET_NAME\n",
        "    %cd $PRISTINE_FOLDER\n",
        "    !for f in *.TIF; do  echo \"Converting $f\"; convert -quality 100 \"$f\"  \"$(basename \"$f\" .TIF).jpg\"; done\n",
        "    !rm *.TIF\n",
        "    %cd ..\n",
        "    %cd $FORGERED_FOLDER\n",
        "    !for f in *.TIF; do  echo \"Converting $f\"; convert -quality 100 \"$f\"  \"$(basename \"$f\" .TIF).jpg\"; done\n",
        "    !rm *.TIF\n",
        "    %cd ..\n",
        "    %cd $GROUND_TRUTH_FOLDER\n",
        "    !for f in *.PNG; do  echo \"Converting $f\"; convert -quality 100 \"$f\"  \"$(basename \"$f\" .PNG).jpg\"; done\n",
        "    !rm *.PNG\n",
        "\n",
        "    # Time computation\n",
        "    print(\"Dataset generation completed in %fs\" % (time.time() - start_time))\n",
        "\n",
        "    %cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAYgXfQ66UHk"
      },
      "source": [
        "if DATASET_NAME == \"CASIA\":\n",
        "\n",
        "    # 1. Download Dataset\n",
        "    start_time = time.time()\n",
        "    %cd /content\n",
        "    !mkdir data\n",
        "    %cd data\n",
        "    !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1IDUgcoUeonBxx2rASX-_QwV9fhbtqdY8' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1IDUgcoUeonBxx2rASX-_QwV9fhbtqdY8\" -O 'CASIA2.0_revised.zip' && rm -rf /tmp/cookies.txt\n",
        "    output.clear()\n",
        "\n",
        "    # 2. Extract Dataset\n",
        "    print(\"Dataset extraction...\")\n",
        "    ZIPFILE = 'CASIA2.0_revised' + '.zip'\n",
        "    !unzip -q $ZIPFILE\n",
        "    !mv CASIA2.0_revised CASIA\n",
        "    !rm CASIA2.0_revised.zip\n",
        "    output.clear()\n",
        "\n",
        "    # 3. Generate GT\n",
        "    print(\"Generating Ground Truth...\")\n",
        "    %cd CASIA\n",
        "    !git clone https://github.com/namtpham/casia2groundtruth.git\n",
        "    !mv /content/data/CASIA/casia2groundtruth/CASIA2.0_Groundtruth.zip /content/data/CASIA/\n",
        "    !rm -r casia2groundtruth\n",
        "    ZIPFILE = 'CASIA2.0_Groundtruth' + '.zip'\n",
        "    !unzip -q $ZIPFILE\n",
        "    !mv CASIA2.0_Groundtruth Gt\n",
        "    !rm CASIA2.0_Groundtruth.zip\n",
        "    output.clear()\n",
        " \n",
        "    # 4. Select randomly 760 images\n",
        "    %cd Tp       \n",
        "    print(\"Selecting 760 images and removing the others...\")\n",
        "    files = os.listdir('.')\n",
        "    indices = random.sample(range(len(files)), k=DATASET_SIZE)\n",
        "    for i, f in enumerate(tqdm(files)):\n",
        "        if i not in indices:\n",
        "            gt_f = '../Gt/' + f.replace(f[-4:], '_gt.png')\n",
        "            !rm $f\n",
        "            !rm $gt_f\n",
        "    output.clear()\n",
        "\n",
        "    # Time computation\n",
        "    print(\"Dataset generation completed in %fs\" % (time.time() - start_time))\n",
        "\n",
        "    %cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i67zyTyZhoHe"
      },
      "source": [
        "if DATASET_NAME == \"Spliced_COCO\":\n",
        "    # 1. Download Dataset\n",
        "    start_time = time.time()\n",
        "    %cd /content\n",
        "    !mkdir data\n",
        "    %cd data\n",
        "    !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=11cp_lomeTQUqRCJslMtXJbSHqCz8ir5P' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=11cp_lomeTQUqRCJslMtXJbSHqCz8ir5P\" -O 'Spliced_COCO.zip' && rm -rf /tmp/cookies.txt\n",
        "    output.clear()\n",
        "\n",
        "    # 2. Extract Dataset\n",
        "    print(\"Dataset extraction...\")\n",
        "    ZIPFILE = 'Spliced_COCO' + '.zip'\n",
        "    !unzip -q $ZIPFILE\n",
        "    !rm Spliced_COCO.zip\n",
        "    %cd /content\n",
        "    output.clear()\n",
        "\n",
        "    # Time computation\n",
        "    print(\"Dataset generation completed in %fs\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkzd7Qcs6cxP"
      },
      "source": [
        "**Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAKnZoGk6b3-"
      },
      "source": [
        "path_img = DATA_DIR + DATASET_NAME + '/' + FORGERED_FOLDER\n",
        "path_mask = DATA_DIR + DATASET_NAME + '/' + GROUND_TRUTH_FOLDER\n",
        "files = os.listdir(path_img)\n",
        "pngs =  [f[:-4] for f in files if f[-3:] == 'png']\n",
        "jpegs = [f[:-4] for f in files if f[-3:] == 'jpg']\n",
        "tiffs = [f[:-4] for f in files if f[-3:] == 'tif']\n",
        "print(len(files), \"files\")\n",
        "print(len(pngs), \"pngs\")\n",
        "print(len(tiffs), \"tiffs\")\n",
        "print(len(jpegs), \"jpegs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm3GXXKC6pDg"
      },
      "source": [
        "from albumentations.augmentations.transforms import (\n",
        "    VerticalFlip, HorizontalFlip, JpegCompression, GaussNoise)\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "ids = [f[:-4] for f in os.listdir(path_img)]\n",
        "random.shuffle(ids)\n",
        "completedata = {\n",
        "    'train': ids[:TRAIN_SIZE], \n",
        "    'val': ids[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE], \n",
        "    'test': ids[TRAIN_SIZE+VAL_SIZE:]\n",
        "}\n",
        "\n",
        "train_set = [f for f in os.listdir(path_img) if f[:-4] in completedata['train']]\n",
        "val_set = [f for f in os.listdir(path_img) if f[:-4] in completedata['val']]\n",
        "test_set = [f for f in os.listdir(path_img) if f[:-4] in completedata['test']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0psjXfb6q0v"
      },
      "source": [
        "# Convert all files possible to jpg\n",
        "for i in tqdm(files):               \n",
        "    name = i.split('.')[0]\n",
        "    img = os.path.join(path_img, i)\n",
        "    image = io.imread(img) \n",
        "    if '.jpg' not in i: \n",
        "        if image.shape[-1] == 4: \n",
        "            image = skimage.color.rgba2rgb(image) \n",
        "        io.imsave(os.path.join(path_img, '{}.jpg'.format(name)), image, quality=100)\n",
        "\n",
        "# Remove all png files\n",
        "if DATASET_NAME == 'CASIA':\n",
        "    !rm /content/data/CASIA/Tp/*.tif\n",
        "elif DATASET_NAME == 'Spliced_COCO':\n",
        "    !rm /content/data/Spliced_COCO/Tp/*.png\n",
        "\n",
        "# Convert unconvertible tiffs (PTIFFS)\n",
        "if len(tiffs) > 0:\n",
        "    %cd /content/data/CASIA/Tp\n",
        "    for f in tiffs:\n",
        "        if f not in jpegs:\n",
        "            f = f + '.tif'\n",
        "            print(\"Converting\", f)\n",
        "            !convert -quality 100 \"$f\"  \"$(basename \"$f\" .tif).jpg\"\n",
        "\n",
        "    ## Remove bad files\n",
        "    print(\"\\n\\nRemoving or replacing bad files...\\n\")\n",
        "    for filename in tqdm(os.listdir('.')):\n",
        "        if '-0' in filename:\n",
        "            newname = filename.replace('-0', '')\n",
        "            !mv $filename $newname\n",
        "        elif '-' in filename:\n",
        "            !rm $filename\n",
        "    !rm /content/data/CASIA/Tp/*.tif\n",
        "\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMv0tGLf6sZA"
      },
      "source": [
        "# See issue https://github.com/yelusaleng/RRU-Net/issues/9\n",
        "# N.B: Don't execute if you want to test Noise and Compression attacks\n",
        "\n",
        "if DATASET_NAME == 'CASIA':\n",
        "    train_set = [f.replace('.tif', '.jpg') for f in train_set]\n",
        "elif DATASET_NAME == 'Spliced_COCO':\n",
        "    train_set = [f.replace('.png', '.jpg') for f in train_set]\n",
        "\n",
        "for i in tqdm(train_set):\n",
        "    name = i.split('.')[0]\n",
        "    img = os.path.join(path_img, i) \n",
        "    mask = os.path.join(path_mask, '{}_gt.png'.format(name)) \n",
        "    image = io.imread(img) \n",
        "    mask = io.imread(mask)\n",
        "\n",
        "    list_quality = [50, 60, 70, 80, 90]\n",
        "    quality = choice(list_quality)\n",
        "    io.imsave(os.path.join(path_img, '{}_q.jpg'.format(name)), image, quality=quality)\n",
        "    io.imsave(os.path.join(path_mask, '{}_q_gt.png'.format(name)), mask)\n",
        "\n",
        "    whatever_data = \"my name\"\n",
        "\n",
        "    augmentation = VerticalFlip(p=1.0)\n",
        "    data = {\"image\": image, \"mask\": mask, \"whatever_data\": whatever_data, \"additional\": \"hello\"}\n",
        "    augmented = augmentation(**data)\n",
        "    image_ver, mask_ver, whatever_data, additional = augmented[\"image\"], augmented[\"mask\"], augmented[\"whatever_data\"], augmented[\"additional\"]\n",
        "    io.imsave(os.path.join(path_img, '{}_ver.jpg'.format(name)), image_ver, quality=100)\n",
        "    io.imsave(os.path.join(path_mask, '{}_ver_gt.png'.format(name)), mask_ver)\n",
        "\n",
        "    augmentation = HorizontalFlip(p=1.0)\n",
        "    data = {\"image\": image, \"mask\": mask, \"whatever_data\": whatever_data, \"additional\": \"hello\"}\n",
        "    augmented = augmentation(**data)\n",
        "    image_hor, mask_hor, whatever_data, additional = augmented[\"image\"], augmented[\"mask\"], augmented[\"whatever_data\"], augmented[\"additional\"]\n",
        "    io.imsave(os.path.join(path_img, '{}_hor.jpg'.format(name)), image_hor, quality=100)\n",
        "    io.imsave(os.path.join(path_mask, '{}_hor_gt.png'.format(name)), mask_hor)\n",
        "\n",
        "    augmentation = GaussNoise(var_limit=(10, 50), p=1.0)\n",
        "    data = {\"image\": image, \"mask\": mask, \"whatever_data\": whatever_data, \"additional\": \"hello\"}\n",
        "    augmented = augmentation(**data)\n",
        "    image_g, mask_g, whatever_data, additional = augmented[\"image\"], augmented[\"mask\"], augmented[\"whatever_data\"], augmented[\"additional\"]\n",
        "    io.imsave(os.path.join(path_img, '{}_G.jpg'.format(name)), image_g, quality=100)\n",
        "    io.imsave(os.path.join(path_mask, '{}_G_gt.png'.format(name)), mask_g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZZjUBGNR7Tk"
      },
      "source": [
        "## Main program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3btI3SAMe_ze"
      },
      "source": [
        "**Dataset initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmYc3bbBrNYO"
      },
      "source": [
        "path_img = DATA_DIR + DATASET_NAME + '/' + FORGERED_FOLDER\n",
        "ids = [f[:-4] for f in os.listdir(path_img)]\n",
        "val_ids = [id for id in ids if (id[-1]!='q' and id[-1]!='r' and id[-1]!='G')]\n",
        "random.shuffle(val_ids)\n",
        "val_ids = val_ids[:VAL_SIZE+TEST_SIZE]\n",
        "train_ids = [id for id in ids if id not in val_ids]\n",
        "test_ids = val_ids[VAL_SIZE:]\n",
        "val_ids = val_ids[:VAL_SIZE]\n",
        "random.shuffle(train_ids)\n",
        "completedata = {'train': train_ids, 'val': val_ids, 'test': test_ids}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC3JRoojulon"
      },
      "source": [
        "print(len(completedata['train']))\n",
        "print(len(completedata['val']))\n",
        "print(len(completedata['test']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDrzPuVs2IJn"
      },
      "source": [
        "def train_dataset_regenerate(ids):\n",
        "    random.shuffle(ids)\n",
        "    train_generator = partial(Forgery_Detection_Dataset(ids).generate, mode='train')\n",
        "    train_dataset = tf.data.Dataset.from_generator(\n",
        "        train_generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=IMG_SHAPE[::-1]+(IMG_CHANNELS,), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=IMG_SHAPE[::-1]+(IMG_CHANNELS,), dtype=tf.float32)\n",
        "        )\n",
        "    )\n",
        "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "    return train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRMwhyFce8Uw"
      },
      "source": [
        "## Train dataset --------------------------------------------------------------------------\n",
        "print(\"Train dataset generation ...\")\n",
        "train_generator = partial(Forgery_Detection_Dataset(train_ids).generate, mode='train')\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    train_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=IMG_SHAPE[::-1]+(IMG_CHANNELS,), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=IMG_SHAPE[::-1]+(IMG_CHANNELS,), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "## Validation dataset ---------------------------------------------------------------------\n",
        "print(\"Validation dataset generation ...\")\n",
        "val_generator = partial(Forgery_Detection_Dataset(val_ids).generate, mode='val')\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "    val_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=IMG_SHAPE[::-1]+(IMG_CHANNELS,), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=IMG_SHAPE[::-1]+(IMG_CHANNELS,), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "## Test dataset ----------------------------------------------------------------------------\n",
        "print(\"Test dataset generation ...\")\n",
        "test_generator = partial(Forgery_Detection_Dataset(test_ids).generate, mode='test')\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    test_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=IMG_SHAPE[::-1]+(IMG_CHANNELS,), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=IMG_SHAPE[::-1]+(IMG_CHANNELS,), dtype=tf.float32),\n",
        "    )\n",
        ")\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCobQL5TR0FM"
      },
      "source": [
        "**Network initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op9ebM8OYTB0"
      },
      "source": [
        "LOAD_MODEL = True\n",
        "CHECKPOINT_PERIOD = 10\n",
        "CHECKPOINT_FILEPATH = '/content/drive/MyDrive/RRU_Checkpoints/3/'\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMqH6CDmRz9b"
      },
      "source": [
        "rru_net = RRU_Net(n_channels=3, n_classes=1)\n",
        "boundaries = [50*len(train_ids)/BATCH_SIZE, 100*len(train_ids)/BATCH_SIZE]\n",
        "values = [LEARNING_RATE, LEARNING_RATE*0.1, LEARNING_RATE*0.01]\n",
        "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
        "rru_optimizer = tfa.optimizers.SGDW(WEIGHT_DECAY, learning_rate_fn, MOMENTUM)\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "checkpoint = tf.train.Checkpoint(rru_net)\n",
        "\n",
        "if LOAD_MODEL:\n",
        "    latest = tf.train.latest_checkpoint(CHECKPOINT_FILEPATH)\n",
        "    rru_net.load_weights(latest)    \n",
        "    # last_epoch = SET IF YOU NEED TO CONTINUE OLD TRAINING\n",
        "    # N.B: FOR THE GRAPHS, DON'T FORGET TO RESTORE ALSO THE FILES TRAIN_LOSSES, MAX_SIGMOIDED AND F1_SCORES\n",
        "else:\n",
        "    last_epoch = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDrGzgsnR5_o"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOwwmBHXR53w"
      },
      "source": [
        "threshold = 0.5\n",
        "epoches = []\n",
        "train_losses = []\n",
        "max_sigmoided = []\n",
        "f1_scores = []\n",
        "\n",
        "for epoch in range(last_epoch, EPOCHES):\n",
        "    print(\"_____ Train Epoch %d _____\" % epoch)\n",
        "    start = time.time()\n",
        "    epoch_loss = 0.\n",
        "    max_sigm_epoch = 0.\n",
        "    train_dataset = train_dataset_regenerate(train_ids)\n",
        "\n",
        "    # ----------------- Train ----------------- #\n",
        "    for i, train_batch in enumerate(train_dataset):\n",
        "\n",
        "        # Get images\n",
        "        train_forgery, train_gt = train_batch\n",
        "        train_gt = tf.image.rgb_to_grayscale(train_gt)\n",
        "        \n",
        "        # Learn\n",
        "        with tf.GradientTape() as rru_tape:\n",
        "            out = rru_net(train_forgery)\n",
        "            out_probs = tf.keras.activations.sigmoid(out)\n",
        "            out_probs_flatten = tf.reshape(out_probs, [-1])\n",
        "            train_gt_flatten = tf.reshape(train_gt, [-1])\n",
        "            train_loss = loss(train_gt_flatten, out_probs_flatten)\n",
        "        rru_gradients = rru_tape.gradient(train_loss, rru_net.trainable_variables)\n",
        "        rru_optimizer.apply_gradients(zip(rru_gradients, rru_net.trainable_variables))\n",
        "        max_mask_value = tf.math.reduce_max(out_probs).numpy()\n",
        "\n",
        "        print(\"Epoch %d | Batch %d - Train loss = %f - Max mask value = %f\" % (epoch, i, train_loss, max_mask_value))\n",
        "        max_sigm_epoch += max_mask_value\n",
        "        epoch_loss += train_loss.numpy()\n",
        "\n",
        "\n",
        "    # ---------------- Evaluate ---------------- #\n",
        "    tot_F_measure = 0.\n",
        "    epoches.append(epoch)\n",
        "    train_losses.append(epoch_loss / i)\n",
        "    max_sigmoided.append(max_sigm_epoch / i)\n",
        "    print(\"Evaluation ...\")    \n",
        "\n",
        "    for j, val_batch in enumerate(val_dataset):\n",
        "\n",
        "        # Get images\n",
        "        val_forgery, val_gt = val_batch\n",
        "        val_gt = tf.squeeze(tf.image.rgb_to_grayscale(val_gt))\n",
        "\n",
        "        for v_forg, v_gt in zip(val_forgery, val_gt):\n",
        "\n",
        "            # Compute rru output mask\n",
        "            v_forg = tf.expand_dims(v_forg, 0)\n",
        "            out = rru_net(v_forg)\n",
        "            out_gray = tf.squeeze(out)\n",
        "            out_mask = tf.cast(tf.keras.activations.sigmoid(out_gray) > threshold, dtype=tf.float32)\n",
        "\n",
        "            # Compute F-measure\n",
        "            v_gt = tf.expand_dims(v_gt, 0)\n",
        "            intersection = tf.math.reduce_sum(tf.math.multiply(out_mask, v_gt))\n",
        "            union = tf.math.reduce_sum(out_mask) + tf.math.reduce_sum(v_gt) + 1e-5\n",
        "            tot_F_measure += 2 * intersection.numpy() / union.numpy()\n",
        "    \n",
        "    f1_scores.append(tot_F_measure / VAL_SIZE)\n",
        "    print(\"Epoch %d completed in %f seconds | medium loss = %f | F1-score = %f\" % (epoch, time.time() - start, train_losses[epoch], f1_scores[epoch]))\n",
        "    \n",
        "    # Plot train process\n",
        "    fig = plt.figure()\n",
        "\n",
        "    plt.title('Training Process')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('value')\n",
        "    l1, = plt.plot(epoches, train_losses, c='red')\n",
        "    l2, = plt.plot(epoches, f1_scores, c='blue')\n",
        "    l3, = plt.plot(epoches, max_sigmoided, c='green')\n",
        "\n",
        "    plt.legend(handles=[l1, l2, l3], labels=['Train Loss', 'F1-score', 'Max sigmoided values'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    if epoch % CHECKPOINT_PERIOD == 0 and epoch > 0:\n",
        "        print(\" - Saving Model...\")\n",
        "        save_path = checkpoint.save(CHECKPOINT_FILEPATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iREl_03AlxfN"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnOgtJxWlwne"
      },
      "source": [
        "tot_F_measure = 0.\n",
        "threshold = 0.5\n",
        "print(\"Evaluation ...\")    \n",
        "\n",
        "for j, val_batch in enumerate(val_dataset):\n",
        "    # Get images\n",
        "    val_forgery, val_gt = val_batch\n",
        "    val_gt = tf.squeeze(tf.image.rgb_to_grayscale(val_gt))\n",
        "\n",
        "    for v_forg, v_gt in zip(val_forgery, val_gt):\n",
        "\n",
        "        # Compute rru output mask\n",
        "        v_forg = tf.expand_dims(v_forg, 0)\n",
        "        out = rru_net(v_forg)\n",
        "        out_gray = tf.squeeze(out)\n",
        "        out_mask = tf.cast(tf.keras.activations.sigmoid(out_gray) > threshold, dtype=tf.float32)\n",
        "\n",
        "        # Show imgs\n",
        "        plt.imshow(tf.squeeze(v_forg))\n",
        "        plt.show()\n",
        "        plt.imshow(v_gt, cmap='gray')\n",
        "        plt.show()\n",
        "        plt.imshow(out_mask, cmap='gray')\n",
        "        plt.show()\n",
        "\n",
        "        # Compute F-measure\n",
        "        v_gt = tf.expand_dims(v_gt, 0)\n",
        "        intersection = tf.math.reduce_sum(tf.math.multiply(out_mask, v_gt))\n",
        "        union = tf.math.reduce_sum(out_mask) + tf.math.reduce_sum(v_gt) + 1e-5\n",
        "        tot_F_measure += 2 * intersection.numpy() / union.numpy()\n",
        "\n",
        "print(\"F1-score =\", tot_F_measure / VAL_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTAfsYjb5XgQ"
      },
      "source": [
        "## Noise and Compression attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJiV10a6B3ZB"
      },
      "source": [
        "from albumentations.augmentations.transforms import (\n",
        "    VerticalFlip, HorizontalFlip, JpegCompression, GaussNoise)\n",
        "import os\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMnZwru8Lmyh"
      },
      "source": [
        "attack = 'Gaussian_Noise'\n",
        "#attack = 'Jpeg_Compression'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vX5ROOrF7sP"
      },
      "source": [
        "path_img = DATA_DIR + DATASET_NAME + '/' + FORGERED_FOLDER\n",
        "ids = [f[:-4] for f in os.listdir(path_img)]\n",
        "val_ids = [id for id in ids if (id[-1]!='q' and id[-1]!='r' and id[-1]!='G')]\n",
        "random.shuffle(val_ids)\n",
        "val_ids = val_ids[:VAL_SIZE+TEST_SIZE]\n",
        "train_ids = [id for id in ids if id not in val_ids]\n",
        "test_ids = val_ids[VAL_SIZE:]\n",
        "val_ids = val_ids[:VAL_SIZE]\n",
        "random.shuffle(train_ids)\n",
        "completedata = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
        "test_set = [f for f in os.listdir(path_img) if f[:-4] in completedata['test']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJnWGhnK6V-b"
      },
      "source": [
        "# Test set initialization\n",
        "if DATASET_NAME == 'CASIA':\n",
        "    test_set = [f.replace('.tif', '.jpg') for f in test_set]\n",
        "elif DATASET_NAME == 'Spliced_COCO':\n",
        "    test_set = [f.replace('.png', '.jpg') for f in test_set]\n",
        "trial_index = 5\n",
        "path_img = DATA_DIR + DATASET_NAME + '/' + FORGERED_FOLDER\n",
        "path_mask = DATA_DIR + DATASET_NAME + '/' + GROUND_TRUTH_FOLDER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOJrYSD-6ZV7"
      },
      "source": [
        "**Noise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_ZP2S_q5X1X"
      },
      "source": [
        "# Noisy data generation\n",
        "if attack == 'Gaussian_Noise':\n",
        "  print(\"\\nGaussian Noise attack with variance =\", NOISE_VARIANCES[trial_index], \"\\n\")\n",
        "  for i in tqdm(test_set):\n",
        "      name = i.split('.')[0]\n",
        "      img = os.path.join(path_img, i) \n",
        "      mask = os.path.join(path_mask, '{}_gt.png'.format(name)) \n",
        "      image = io.imread(img) \n",
        "      mask = io.imread(mask)\n",
        "\n",
        "      whatever_data = \"my name\"\n",
        "\n",
        "      #augmentation = GaussNoise(var_limit=NOISE_VARIANCES[trial_index], p=1.0)\n",
        "      augmentation = GaussNoise(var_limit=0.5, p=1.0)\n",
        "      data = {\"image\": image, \"mask\": mask, \"whatever_data\": whatever_data, \"additional\": \"hello\"}\n",
        "      augmented = augmentation(**data)\n",
        "      image_g, mask_g, whatever_data, additional = augmented[\"image\"], augmented[\"mask\"], augmented[\"whatever_data\"], augmented[\"additional\"]\n",
        "      io.imsave(os.path.join(path_img, '{}_G.jpg'.format(name)), image_g, quality=100)\n",
        "      io.imsave(os.path.join(path_mask, '{}_G_gt.png'.format(name)), mask_g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWKbINCI62pT"
      },
      "source": [
        "**Compression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgga0Uhz6Md0"
      },
      "source": [
        "# Compressed data generation\n",
        "if attack == 'Jpeg_Compression':\n",
        "  print(\"\\nJPEG Compression attack with quality =\", COMPRESSION_QUALITIES[trial_index], \"\\n\")\n",
        "  for i in tqdm(test_set):\n",
        "      name = i.split('.')[0]\n",
        "      img = os.path.join(path_img, i) \n",
        "      mask = os.path.join(path_mask, '{}_gt.png'.format(name)) \n",
        "      image = io.imread(img) \n",
        "      mask = io.imread(mask)\n",
        "\n",
        "      quality = COMPRESSION_QUALITIES[trial_index]\n",
        "      io.imsave(os.path.join(path_img, '{}_q.jpg'.format(name)), image, quality=quality)\n",
        "      io.imsave(os.path.join(path_mask, '{}_q_gt.png'.format(name)), mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3NIFogy6-fp"
      },
      "source": [
        "**Evaluation on the corrupted test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvrSbaDQMNJr"
      },
      "source": [
        "test_ids = [f[:-4] for f in os.listdir(path_img) if f[:-6] in completedata['test']]\n",
        "\n",
        "## Test dataset ----------------------------------------------------------------------------\n",
        "print(\"Test dataset generation ...\")\n",
        "test_generator = partial(Forgery_Detection_Dataset(test_ids).generate, mode='test')\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    test_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=IMG_SHAPE[::-1]+(IMG_CHANNELS,), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=IMG_SHAPE[::-1]+(IMG_CHANNELS,), dtype=tf.float32),\n",
        "    )\n",
        ")\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXgTb-L96-L6"
      },
      "source": [
        "tot_F_measure = 0.\n",
        "threshold = 0.5\n",
        "print(\"Evaluation ...\")    \n",
        "\n",
        "for j, test_batch in enumerate(test_dataset):\n",
        "    # Get images\n",
        "    test_forgery, test_gt = test_batch\n",
        "    test_gt = tf.squeeze(tf.image.rgb_to_grayscale(test_gt))\n",
        "\n",
        "    for t_forg, t_gt in zip(test_forgery, test_gt):\n",
        "\n",
        "        # Compute rru output mask\n",
        "        t_forg = tf.expand_dims(t_forg, 0)\n",
        "        out = rru_net(t_forg)\n",
        "        out_gray = tf.squeeze(out)\n",
        "        out_mask = tf.cast(tf.keras.activations.sigmoid(out_gray) > threshold, dtype=tf.float32)\n",
        "\n",
        "        # Show images\n",
        "        plt.imshow(tf.squeeze(t_forg))\n",
        "        plt.show()\n",
        "        plt.imshow(t_gt, cmap='gray')\n",
        "        plt.show()\n",
        "        plt.imshow(out_mask, cmap='gray')\n",
        "        plt.show()\n",
        "\n",
        "        # Compute F-measure\n",
        "        t_gt = tf.expand_dims(t_gt, 0)\n",
        "        intersection = tf.math.reduce_sum(tf.math.multiply(out_mask, t_gt))\n",
        "        union = tf.math.reduce_sum(out_mask) + tf.math.reduce_sum(t_gt) + 1e-5\n",
        "        tot_F_measure += 2 * intersection.numpy() / union.numpy()\n",
        "\n",
        "print(\"F1-score =\", tot_F_measure / TEST_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzhALWWfUKi0"
      },
      "source": [
        "# Remove augmented images\n",
        "%cd $path_img\n",
        "for f in os.listdir(path_img):\n",
        "    id = f.split('.')[0]\n",
        "    if id[-1] == 'q':\n",
        "      !rm $f\n",
        "%cd $path_mask\n",
        "for f in os.listdir(path_mask):\n",
        "    id = f.split('.')[0]\n",
        "    if id[-4] == 'q':\n",
        "      !rm $f"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}